\chapter{Introduction}
\label{chap:introduction}

Broad overview of goals for digital evolution models
Talk about concept of scalability
Without isolation -> with dynamic interactions
Start each section by explaining what the section title means (put folks into context)
Overview the goals of this dissertation at a conceptual level AND give thesis statement
Sentence or short paragraph
Emphasize computational and evolutionary biology contributions (dual degree...)
Both computational (hardware/software; designing AND implementing a model system) limitations and theoretical (underlying evolutionary dynamics)
Lots of challenges with advancing our understanding of major evolutionary transitions in individuality.
Digital evolution has advanced our ability to experimentally investigate evolutionary questions, but for some questions (particularly those that involve huge numbers of cells/agents) we are currently computationally limited.
In this dissertation I will demonstrate techniques that I designed in order to overcome those limitations, and then use these new abilities to design a digital evolution system capable of allowing organisms to undergo transitions in individuality.
I will also use this system to investigate specific questions associated with those transitions.

\section{Scalability without Isolation is Critical for Future Digital Evolution Models}
[
[Traditional digital evolution systems implement scalability by creating many isolated islands of organisms, that only interact periodically.
This is not ideal for studying evolutionary dynamics because…]]]
Digital evolution techniques complement traditional wet-lab evolution experiments by enabling researchers to address questions that would be otherwise limited by:
\begin{itemize}
\item reproduction rate (which determines the number of generations that can be observed in a set amount of time),
\item incomplete observations (every event in a digital system can be tracked),
\item physically-impossible experimental manipulations (every event in a digital system can can be arbitrarily altered), or
\item resource- and labor-intensity (digital experiments and assays can be easily automated).
\end{itemize}
The versatility and rapid generational turnover of digital systems can easily engender a notion that such systems can already operate at scales greatly exceeding biological evolution experiments.
Although digital evolution techniques can feasibly simulate populations numbering in the millions or billions, very simple agents and/or very limited agent-agent interaction.
With more complex agents controlled by genetic programs, neural networks, or the like, feasible population sizes dwindle down to thousands or hundreds of agents.

\section{Putting Scale in Perspective}

Take Avida as an example. This popular software system that enables experiments with evolving self-replicating computer programs.
In this system, a population of ten thousand can undergo about twenty thousand generations per day.
This means that about two hundred million replication cycles are performed in a day \citep{ofria2009artificial}.
Each flask in the Lenski Long-Term Evolution Experiment hosts a similar number of replication cycles.
In their system, E. coli undergo about six doublings per day.
Effective population size is reported as 30 million \citep{good2017dynamics}. Hence, about 180 million replication cycles elapse per day.
Likewise, in Ratcliff’s work studying the evolution of multicellularity in S. cerevisiae, about six doublings per day occur among a population numbering on the order of a billion cells \citep{ratcliff2012experimental}.
So, around six billion cellular replication cycles elapse per day in this system.

Although artificial life practitioners traditionally describe instances of their simulations as “worlds,” with serial processing power their scale aligns (in naive terms) more along the lines of a single flask.
Of course, such a comparison neglects the profound disparity between Avidians and bacteria or yeast in terms of genome information content, information content of cellular state, and both quantity and diversity of interactions with the environment and with other cells.
Recent work with SignalGP has sought to address some of these shortcomings by developing digital evolution substrates suited to more dynamic environmental and agent-agent interactions \citep{lalejini2018evolving} that more effectively incorporate state information \citep{lalejini2021tag,lalejini2020case, moreno2019evaluating}.
However, to some degree, more sophisticated and interactive evolving agents will necessarily consume more CPU time on a per-replication-cycle basis --- further shrinking the magnitude of experiments tractable with serial processing.

\section{The Future is Parallel}

Throughout the 20th century, serial processing enjoyed regular advances in computational capacity due to quickening clock cycles, burgeoning RAM caches, and increasingly clever packing together of instructions during execution.
Since, however, performance of serial processing has bumped up against apparent fundamental limits to computing’s current technological incarnation \citep{sutter2005free}.
Instead, advances in 21st century computing power have arrived via multiprocessing \citep[p.~55]{hennessy2011computer} and hardware acceleration (e.g., GPU, FPGA, etc.) \citep{che2008accelerating}.
Contemporary high-performance computing clusters link multiprocessors and accelerators with fast interconnects to enable coordinated work on a single problem \citep[p.~436]{hennessy2011computer}.
High-end clusters already make hundreds of thousands or millions of cores available. More loosely-affiliated banks of servers can also muster significant computational power.
For example, Sentient Technologies notably employed a distributed network of over a million CPUs to run evolutionary algorithms \citep{miikkulainen2019evolving}.
The availability of orders of magnitude greater parallel computing resources in ten and twenty years’ time seems probable, whether through incremental advances with traditional silicon-based technology or via emerging, unconventional technologies such as bio-computing \citep{benenson2009biocomputers} and molecular electronics \citep{xiang2016molecular}.
Such emerging technologies could make greatly vaster collections of computing devices feasible, albeit at the potential cost of component-wise speed \citep{bonnet2013amplifying, ellenbogen2000architectures} and perhaps also component-wise reliability.

\section{What of Scale?}

Digital evolution practitioners have a rich history of leveraging distributed hardware.
It is common practice to distribute multiple self-isolated instantiations of evolutionary runs over multiple hardware units.
In scientific contexts, this practice yields replicate datasets that provide statistical power to answer research questions \citep{dolson2017spatial}.
In applied contexts, this practice yields many converged populations that can be scavenged for the best solutions overall \citep{hornby2006automated}.
Another established practice is to use ``island models'' where individuals are transplanted between populations that are otherwise independently evolving across distributed hardware.
Koza and collaborators’ genetic programming work with a 1,000-cpu Beowulf cluster typifies this approach \citep{bennett1999building}.

In recent years, Sentient Technologies spearheaded digital evolution projects on an unprecedented computational scale, comprising over a million CPUs and capable of a peak performance of 9 petaflops \citep{miikkulainen2019evolving}.
According to its proponents, the scale and scalability of this DarkCycle system was a key aspect of its conceptualization \citep{gilbert2015artificial}.
Much of the assembled infrastructure was pieced together from heterogeneous providers and employed on a time-available basis \citep{blondeau2009distributed}.
Unlike the  model where selection events are performed independently on each CPU, this scheme transferred evaluation criteria between computational instances (in addition to individual genomes) \citep{hodjat2013distributed}.

Sentient Technologies also accelerated the deep learning training process by using many massively-parallel hardware accelerators (e.g., 100 GPUs) to evaluate the performance of candidate neural network architectures on image classification, language modeling, and image captioning problems \citep{miikkulainen2019evolving}.
Analogous work parallelizing the evaluation of an evolutionary individual over multiple test cases in the context of genetic programming has used GPU hardware and vectorized CPU operations
\citep{harding2007fast_springer, langdon2019continuous}.

Existing applications of concurrent approaches to digital evolution distribute populations or individuals across hardware to process them with minimal interaction.
Task independence facilitates this simple, efficient implementation strategy, but precludes application on elements that are not independent.
Parallelizing evaluation of a single individual often emphasizes data-parallelism over independent test cases, which are subsequently consolidated into a single fitness profile.
With respect to model parallelism, Harding has notably applied GPU acceleration to cellular automata models of artificial development systems, which involve intensive interaction between spatially-distributed instantiation of a genetic program \citep{harding2007fast_ieee}.
However, in systems where evolutionary individuals themselves are parallelized they are typically completely isolated from each other.

We argue that, in a manner explicitly accommodating capabilities and limitations of available hardware, open-ended evolution should prioritize dynamic interactions between simulation elements situated across physically distributed hardware components.

\section{Leveraging Distributed Hardware for Open-Ended Evolution}

Unlike most existing applications of distributed computing in digital evolution, open-ended evolution researchers should prioritize dynamic interactions among distributed simulation elements.
Parallel and distributed computing enables larger populations and metapopulations.
However, ecologies, co-evolutionary dynamics, and social behavior all necessitate dynamic interactions among individuals.

Distributed computing should also enable more computationally intensive or complex individuals.
Developmental processes and emergent functionality necessitate dynamic interactions among components of an evolving individual.
Even at a scale where individuals remain computationally tractable on a single hardware component, modeling them as a collection of discrete components configured through generative development (i.e., with indirect genetic representation) can promote scalable properties \citep{lipson2007principles} such as modularity, regularity, and hierarchy \citep{hornby2005measuring, clune2011performance}.
Developmental processes may also promote canalization \citep{stanley2003taxonomy}, for example through exploratory processes and compensatory adjustments \citep{gerhart2007theory}.
To reach this goal, David Ackley has envisioned an ambitious design for modular distributed hardware at a theoretically unlimited scale \citep{ackley2011pursue} and demonstrated an algorithmic substrate for emergent agents that can take advantage of it \citep{ackley2018digital}.

\section{A Path of Expanding Computational Scale}

While by no means certain, the idea that orders-of-magnitude increases in compute power will open up qualitatively different possibilities with respect to open-ended evolution is well founded.
Spectacular advances achieved with artificial neural networks over the last decade illuminate a possible path toward this outcome. As with digital evolution, artificial neural networks (ANNs) were traditionally understood as a versatile, but auxiliary methodology — both techniques were described as “the second best way to do almost anything” \citep{miaoulis2008intelligent,eiben2015introduction}.
However, the utility and ubiquity of ANNs has since increased dramatically. The development of AlexNet is widely considered pivotal to this transformation. AlexNet united methodological innovations from the field (such as big datasets, dropout, and ReLU) with GPU computing that enabled training of orders-of-magnitude-larger networks.
In fact, some aspects of their deep learning architecture were expressly modified to accommodate multi-GPU training \citep{krizhevsky2012imagenet}.
By adapting existing methodology to exploit commercially available hardware, AlexNet spurred the greater availability of compute resources to the research domain and eventually the introduction of custom hardware to expressly support deep learning \citep{jouppi2017datacenter}.

Similarly, progress toward realizing artificial life systems with indefinite scalability seems likely to unfold as incremental achievements that spur additional interest and resources in a positive feedback loop with the development of methodology, software, and eventually specialized hardware to take advantage of those resources.
In addition to developing hardware-agnostic theory and methodology, we believe that pushing the envelope of open-ended evolution will analogously require designing systems that leverage existing commercially-available parallel and distributed compute resources at circumstantially-feasible scales.

\section{Thesis Statement}

How can we build a scalable multicellular digital artificial life system to study key phenomena associated with the concept of open-ended evolution: novelty, complexity, and adaptation?

\section{Outline}

TODO Chapter by chapter overview of what’s in the rest of it
