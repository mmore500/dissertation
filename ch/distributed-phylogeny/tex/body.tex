\chapter{Infrastructure: Distributed Phylogenetic Tracking}
\label{ch:distributed-phylogeny}

\noindent
% Authors: Matthew Andres Moreno, Santiago Rodriguez Papa, and Charles Ofria \\
This chapter is a proposal.

In traditional digital evolution experiments, phylogenetic trees can be tracked perfectly as they progress \citep{bohm2017mabe,wang2018vine,lalejini2019data} rather than reconstructed afterward, as must be done in most biological studies of evolution.
Such direct phylogenetic tracking enables a dizzying suite of experimental possibilities.
In a shared-memory context, it is not difficult to maintain a complete phylogeny where offspring simply track their parent (or vice versa).
As simulations progress for longer durations, memory usage will typically grow at least linearly if population sizes are fixed, or worse if populations are growing or organisms are becoming more complex.
These problems are easily ameliorated, however, by performing garbage collection on extinct lineages and saving older history to disk \citep{bohm2017mabe,dolson2019modes}.

Phylogenetic tracking in a distributed context, where linages may traverse many machines, requires algorithmic adjustment.
However, if sufficient memory or disk space can be afforded to log all reproduction events, recording a perfect phylogeny in a distributed context is still not especially difficult.
Processes could maintain records of each reproduction event, storing the parent organism (and its associated process) with all generated offspring (and their destination processes).
As long as no data goes missing and organisms are uniquely identified globally, these ``dangling ends'' could be joined in postprocessing to weave a continuous global phylogeny.
Of course, for the huge population sizes made possible by distirbuted systems, such stitching may become a demanding task in and of itself.

However, if memory and disk space are limited, distributed phylogeny tracking becomes a more burdonsome challenge.
A naive approach might employ a server model to maintain a central store of phylogenetic data.
Processes would dispatch notifications of birth and death events to the server, which would curate (and gabage collect) phylogenetic history much the same as current serial phylogenetic tracking implementations.
However, this server model approach would present profound scalability challenges: communication and computation burden on the server process would worsen in direct proportion to processor count, and processes would frequently need to pause while waiting for confirmation that a reproduction event was successfully reported.
As organisms became more complex, large genome size could further worsen the communication burden.
A more scalable approach would record birth and death events only on the process(ds) where they unfold.
However, even if it went extinct locally, any lineage that had dispatched offspring to neighboring processes could not be safely garbage collected until the extinction of that dispatched offspring's lineage could be confirmed.
So, garbage collection would require involve extinction notifications to wind back across processes each lineage had traversed.


Under a best-effort model, no system of phylogeny tracking can guarantee proper garbage collection.
For example, it would be possible for an offspring that was dispatched to a neighboring process to have failed to arrive, and both processes would need to keep going, dispite the failure.
So, extinction notifications for the lineage founded by that offspring would never be dispatched --- putting in motion a memory leak of un-garbabe-collectable phylogenetic history.
The situation becomes even more leaky when the possibility of extinction notifications themselves being lost.

In a distributed context --- especially, a distributed, best-effort context --- phylogenetic reconstruction (as opposed to tracking) could prove simpler and more efficient at runtime while providing sufficient power to address experimental questions of interest.
However, phylogenetic reconstruction poses its own difficulties, including
\begin{itemize}
\item accounting for heterogeneity in evolutionary rates (i.e., the rate at which mutations accumulate due to divergent mutation rates or selection pressures) between lineages \citep{lack2010identifying},
\item performing sequence alignment \citep{casci2008lining},
\item mutational saturation \citep{hagstrom2004using},
\item selecting and implementing (or using) complex and diverse algorithmic approaches \citep{kapli2020phylogenetic}, and
\item computational intensity \citep{sarkar2010hardware}.
\end{itemize}

However, the computational basis of digital artificial life experiments provides a unique opportunity: to design genome annotations that are updated with each reproduction event so as to simplify and strengthen phylogenetic reconstruction efforts.
For maximum applicability of our solution, these annotations are phenotypically neutral heritable instrumentation \citep{stanley2002evolving} and do not affect any encodings for functional genome content.
In this chapter, I propose a genome component that enables us to estimate the number of generations elapsed since two extant organisms last shared common ancestor within a guaranteed-width window with near certainty.
Notably, in cases where different evolutionary distances exist between each extant organisms being compared and their common ancestor (e.g., due to one lineage having faster generational turnover), evolutionary distance back to the common ancestor is estimated independently for each extant organisms.
By scaling the width of this window exponentially with elapsed generations (i.e., the window for more ancient common ancestors is more coarse), we can achieve space complexity that scales logarithmically with the length of evolutionary history (and linearly with population size).

\section{Proposed Approach}

The proposed approach annotates genomes with a \texttt{vector} of \texttt{uint64\_t} ``fingerprints.''
Ancestral organisms begin with empty \texttt{vector}s.
In the simplest form of this algorithm, offspring inherit their parent's fingerprints and extend it by one randomly-generated fingerprint.
If two organisms share no common fingerprints, then it is guaranteed that they share no common ancestor.
If some fingerprints are in common, then the distance of each organism back to the common ancestor is simply the count of fingerprints following the first uncommon fingerprint.
The probability of two \texttt{uint64\_t}'s colliding is infinitesimal enough as to be ignored in most circumstances, $5.42\times10^{-20}$.
In the extremely unlikely event that a single collision does occur under this scheme it will only shift our estimate of evolutionary distance by one generation.

However, as currently stated, under this algorithm fingerprints will accumulate proportionally to the length of evolutionary history simulated.
In an evolutionary run with tens of thousands of generations, this approach would soon become intractable.
To solve this problem, we can sacrifice precision pinpointing more ancient evolutionary events by dropping select fingerprints our \texttt{vector}.
Intuitively, if we drop a fingerprint at all generational steps back that are not powers of 2, memory use will grow $O(\log n)$ with generations elapsed.
But how to we pick which fingerprints to drop?

Only fingerprints that were generated at the same generation are meaningful to compare.
Put another way, the process of dropping fingerprints must must align preserved fingerprints in terms of generations measuring forward since the genesis organism, not generations measuring backward since the extant organism.
However, this technique turns out to work if we pick drop indexes according to OEIS Sequence A001511 \cite{sloane2021a001511}.

To provide additional information, we also annotated the genomes with the specific generation ID of the associated organism (now that the number of fingerprints no longer measures this value).
As an extension, simulation time at which each organism was born could also be store to allow evolutionary history to be characterized in terms of updates in addition to generations.
\footnote{
In a best effort system, a global ``update''-like count can be maintained by having processes continually increment and broadcast their update count and adopting the highest they are aware of.
}.

\section{Proposed Work}

A prototype of this approach has been implemented \url{https://github.com/mmore500/dishtiny/blob/incoming/include/dish2/genome/PhyloFingerprints.hpp} and preliminarily tested \url{https://github.com/mmore500/dishtiny/blob/incoming/tests/dish2/genome/PhyloFingerprints.cpp}.

To complete this chapter, I will present this approach in more detail, derive its properties more formally, and --- if possible --- generalize it to allow adjustment of the granularity of history preserved (i.e., tuning the exponential rate at which the estimation window grows).

\section{Contribution}

To our knowledge, this chapter provides a novel solution to a unexplored problem (designing digital genome components that enable phylogenetic reconstruction in the absence of phylogenetic tracking).

Such techniques will be essential for artificial life experiments that use distributed and best-effort computing approaches.
These methods will enable phylogentic analyses in distributed, best-effort systems while preserving those systems' efficiency and scalability.
As parallel and distributed computing becomes increasingly ubiquitous and begins to more widely pervade artificial life systems, this will be a useful technique in the toolbox.
These tenchiques may, in addition, serve some use in traditional serial artificial life simulations as a stopgap in the absence of infrastructure for proper phylogenetic tracking.

Finally, the problem of designing genomes to maximize phlogenetic reconstructability rasies unique questions about phylogenetic estimation.
Such a backward problem --- optimizing genomes to make analyses trivial as opposed to the usual process of optimizing analyses to genomes --- puts questions about the genetic information analyses operate on in a new light.
In particular, the problem of maximum-reconstructability is well suited to extension to derive results on best-case certain bounds of reconstruction algorithms.
In future work, it may be especially interesting to consider the problem of maximizing reconstructibility for a fixed-size genetic component.
