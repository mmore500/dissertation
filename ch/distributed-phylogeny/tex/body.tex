\chapter{Infrastructure: Distributed Phylogenetic Tracking}
\label{ch:distributed-phylogeny}

\noindent
% Authors: Matthew Andres Moreno, Santiago Rodriguez Papa, and Charles Ofria \\
This chapter is in proposal.


In traditional artificial life experiments, phylogenetic trees can be tracked directly as they progress rather than reconstructed afterward, as is typical in most biological studies of evolution.
Such perfect phylogenetic tracking enables a dizzying suite of experimental possibilities.
Indeed, many artificial life and digital evolution systems provide faculties for phylogenetic tracking \citep{bohm2017mabe,wang2018vine,lalejini2019data}.
In a shared-memory context, it is not so difficult to maintain a complete phylogeny where offspring simply hold pointers to the address of their parent (or vice versa).
In long-running simulations, though, some care must be taken with garbage collection of extinct lineages.
Otherwise, memory usage will grow $O(n)$ with elapsed generations (assuming a fixed population size).
Depending on particular needs, phylogenies can also be pruned more aggressively --- for example, to only maintain history reaching back a certain number of generations \citep{dolson2019modes}.

If sufficient memory or disk space can be afforded to forgo garbage collection of extinct lineages, recording a perfect phylogeny in a distributed context isn't so difficult, either.
Every reproduction event would be logged independently by each process.
In particular, it would be critical for processes to track of the parent organism and the source process of incoming offspring as well as the destination processes for outgoing offspring.
These ``dangling ends'' could be joined in postprocessing to weave a continuous global phylogeny.
Although for extreme processor counts and long experiments such stitching may become a demanding task in and of itself, it can be done.

However, with the requirement for garbage collection of extinct lineages to ensure resource demands remain tractable, distributed phylogeny tracking becomes more difficult.
A naive approach might employ a server model to maintain a central store of phylogenetic data.
Processes would dispatch notifications of birth and death events to the server, which would curate (and gabage collect) phylogenetic history much the same as current serial phylogenetic tracking implementations.
However, this server model approach would present profound scalability challenges: communication and computation burden on the server process would worsen in direct proportion to processor count.
Large genome size could further worsen the communication burden.

A more scalable approach would restrict phylogeny-tracking work as much as possible to within the very process where birth and death events unfold.
However, lineages reaching across processing elements pose a key challenge.
Consider a lineage that goes extinct within a particular processor's simulation slice.
Any elements of that lineage that had dispatched offspring to neighboring processes would have to be maintained until the extinction of that dispatched offspring's lineage could be confirmed.
So, garbage collection of a lineage would require involve extinction notifications to wind back across as many processes as that lineage had traversed.

Under a best-effort model, it would be possible for an offspring that was dispatched to a neighboring process to have failed to arrive.
So, extinction notifications for the lineage founded by that offspring would never be dispatched --- putting in motion something like a memory leak of un-garbabe-collectable phylogenetic history.
The situation becomes even more leaky when the possibility of extinction notifications themselves being lost.

In a distributed context --- especially, a distributed, best-effort context ---phylogenetic reconstruction (as opposed to tracking) could prove simpler and more efficient at runtime while providing sufficient power to address experimental questions of interest.
However, phylogenetic reconstruction poses its own difficulties, including
\begin{itemize}
\item accounting for heterogeneity in evolutionary rates (i.e., the rate at which mutations accumulate due to divergent mutation rates or selection pressures) between lineages \citep{lack2010identifying},
\item performing sequence alignment \citep{casci2008lining},
\item mutational saturation \citep{hagstrom2004using},
\item selecting and implementing (or using) complex and diverse algorithmic approaches \citep{kapli2020phylogenetic}, and
\item computational intensity \citep{sarkar2010hardware}.
\end{itemize}

However, the computational basis of digital artificial life experiments provides a unique opportunity: to design genomes and mutation in order to simplify and strengthen phylogenetic reconstruction efforts.
To simplify the problem and maximize the applicability of our solution, we will focus on an approach that augments arbitrary existing functional genome components with phenotypically neutral heritable instrumentation \citep{stanley2002evolving} (as opposed to wading into encodings for functional genome content).
This chapter proposes a genome component that enables the number of generations elapsed between each of two extant organisms and their common ancestor to be estimated within a certain window with near certainty.
Importantly in cases of heterogeneous evolutionary rates, estimates of evolutionary distance between both extant organisms and the common ancestor are separate.
By scaling the width of this window exponentially with elapsed generations (i.e., the window for more ancient common ancestors is more coarse), we can achieve space complexity that scales logarithmically with the length of evolutionary history.

\section{Proposed Approach}

The proposed approach equips genomes with a \texttt{vector} of \texttt{uint64\_t} ``fingerprints.''
Ancestral organisms begin with empty \texttt{vector}s.
Each time a generation elapses, offspring inherit their parent's fingerprints and extend it by one randomly-generated fingerprint.
If two organisms share no common fingerprints, then they share no common ancestor.
If some fingerprints are in common, then to measure distance of each organism back to the common ancestor simply count of fingerprints held after the first uncommon fingerprint.
The probability of a two \texttt{uint64\_t}'s colliding is infinitesimal enough as to be ignored in most circumstances, $5.42\times10^{-20}$.
Plus, if a single collision does occur under this scheme it will only shift our estimate of evolutionary distance by one generation.

However, as currently stated, fingerprints will accumulate proportionally to the length of evolutionary history simulated.
In an evolutionary run with tens of thousands of generations, this would soon become intractable.
To solve this problem, we can sacrifice precision pinpointing more ancient evolutionary events by dropping ancient fingerprints our \texttt{vector}.
If we drop a fingerprint at all generations that are not powers of 2, memory use will grow $O(\log n)$ with generations elapsed.

Picking out which fingerprints to drop, though, is tricky.
Only fingerprints that were generated at the same generation are meaningful to compare.
Put another way, the process of dropping fingerprints must must align preserved fingerprints in terms of generations measuring forward since the genesis organism, not generations measuring backward since the extant organism.
However, it turns out to work if we pick drop indexes according to OEIS Sequence A001511 \cite{sloane2021a001511}.

As an extension, current simulation time could be stored alongside fingerprints when generated to allow evolutionary history to be characterized in terms of updates in addition to generations.
(In a best effort system, a global ``update''-like count can be maintained by having processes continually increment and broadcast their update count and adopting the highest they are aware of.)

\section{Proposed Work}

A prototype of this approach has been implemented \url{https://github.com/mmore500/dishtiny/blob/incoming/include/dish2/genome/PhyloFingerprints.hpp} and preliminarily tested \url{https://github.com/mmore500/dishtiny/blob/incoming/tests/dish2/genome/PhyloFingerprints.cpp}.

To complete this chapter, I will present this approach in more detail, derive its properties more formally, and --- if possible --- generalize it to allow adjustment of the granularity of history preserved (i.e., tuning the exponential rate at which the estimation window grows).

\section{Contribution}

To our knowledge, this chapter provides a novel solution to a unexplored problem (designing digital genome components that enable phylogenetic reconstruction in the absence of phylogenetic tracking).

Such techniques will be essential for artificial life experiments that use distributed and best-effort computing approaches.
These methods will enable phylogentic analyses in distributed, best-effort systems while preserving those systems' efficiency and scalability.
As parallel and distributed computing becomes increasingly ubiquitous and begins to more widely pervade artificial life systems, this will be a useful technique in the toolbox.
These tenchiques may, in addition, serve some use in traditional serial artificial life simulations as a stopgap in the absence of infrastructure for proper phylogenetic tracking.

Finally, the problem of designing genomes to maximize phlogenetic reconstructability should be enjoyed as a curiosity of sorts by computational and mathematical phylogenetics researchers.
Such a backward problem --- optimizing genomes to make analyses trivial as opposed to the usual process of optimizing analyses to genomes --- puts questions about the genetic information analyses operate on in a new light.
In future work, it may be especially interesting to consider the problem of maximizing reconstructibility for a fixed-size genetic component.
